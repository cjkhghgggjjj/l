import os
import subprocess
import sys
import traceback

# ===========================
# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
# ===========================
def install_packages():
    required_libs = [
        "flask",
        "onnxruntime", 
        "opencv-python-headless",
        "numpy",
        "requests",
        "pillow"
    ]
    
    print("ğŸ”§ Ø¨Ø¯Ø¡ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©...")
    
    for lib in required_libs:
        try:
            if lib == "opencv-python-headless":
                __import__("cv2")
            elif lib == "pillow":
                __import__("PIL")
            else:
                __import__(lib.split('-')[0])
            print(f"âœ… {lib} - Ù…Ø«Ø¨Øª Ù…Ø³Ø¨Ù‚Ø§Ù‹")
        except ImportError:
            print(f"ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ«Ø¨ÙŠØª {lib}...")
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", lib, "--quiet"])
                print(f"âœ… ØªÙ… ØªØ«Ø¨ÙŠØª {lib} Ø¨Ù†Ø¬Ø§Ø­")
            except Exception as e:
                print(f"âŒ ÙØ´Ù„ ØªØ«Ø¨ÙŠØª {lib}: {e}")

install_packages()

# ===========================
# Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
# ===========================
print("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ«Ø¨ÙŠØª...")

try:
    from flask import Flask, render_template_string, request, send_file, jsonify
    import cv2
    import numpy as np
    import requests
    import onnxruntime as ort
    from PIL import Image
    import io
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
except ImportError as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª: {e}")
    sys.exit(1)

# ===========================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ===========================
app = Flask(__name__)

print("ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ AntelopeV2...")

# Ø±ÙˆØ§Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ AntelopeV2
MODEL_URLS = {
    "detection": "https://classy-douhua-0d9950.netlify.app/scrfd_10g_bnkps.onnx.index.js",
    "recognition": "https://classy-douhua-0d9950.netlify.app/glintr100.onnx.index.js",
    "genderage": "https://classy-douhua-0d9950.netlify.app/genderage.onnx.index.js",
    "landmarks_2d": "https://classy-douhua-0d9950.netlify.app/2d106det.onnx.index.js",
    "landmarks_3d": "https://classy-douhua-0d9950.netlify.app/1k3d68.onnx.index.js"
}

class AntelopeV2FaceAnalysis:
    """ÙØ¦Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ AntelopeV2"""
    
    def __init__(self):
        self.sessions = {}
        self.initialized = False
        self.providers = ['CPUExecutionProvider']
    
    def load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ AntelopeV2 Ù…Ù† Ø§Ù„Ù€ API"""
        try:
            print("ğŸŒ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ AntelopeV2 Ù…Ù† API...")
            
            models_to_load = {
                "detection": "ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡",
                "recognition": "Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡", 
                "genderage": "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø³ ÙˆØ§Ù„Ø¹Ù…Ø±",
                "landmarks_2d": "Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 2D",
                "landmarks_3d": "Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 3D"
            }
            
            for model_key, model_name in models_to_load.items():
                print(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ {model_name}...")
                response = requests.get(MODEL_URLS[model_key], timeout=60)
                response.raise_for_status()
                
                # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ONNX Runtime
                self.sessions[model_key] = ort.InferenceSession(
                    response.content, 
                    providers=self.providers
                )
                
                print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {model_name} Ø¨Ù†Ø¬Ø§Ø­")
                
                # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                session = self.sessions[model_key]
                print(f"   ğŸ” Ù…Ø¹Ù„ÙˆÙ…Ø§Øª {model_name}:")
                for i, input in enumerate(session.get_inputs()):
                    print(f"      Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ {i}: {input.name} - {input.shape} - {input.type}")
                for i, output in enumerate(session.get_outputs()):
                    print(f"      Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ {i}: {output.name} - {output.shape} - {output.type}")
            
            self.initialized = True
            print("ğŸ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ AntelopeV2 Ø¨Ù†Ø¬Ø§Ø­!")
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
            traceback.print_exc()
            return False
    
    def prepare(self, ctx_id=0, det_size=(640, 640)):
        return self.load_models()
    
    def get(self, img):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ AntelopeV2"""
        if not self.initialized:
            return []
        
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ RGB
            if len(img.shape) == 3:
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            else:
                img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
            
            original_height, original_width = img.shape[:2]
            
            # ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡
            faces = self._detect_faces(img_rgb, original_width, original_height)
            
            # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ ÙˆØ¬Ù‡
            for face in faces:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ embedding
                face.embedding = self._get_face_embedding(img_rgb, face.bbox)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø³ ÙˆØ§Ù„Ø¹Ù…Ø±
                face.gender, face.age = self._analyze_gender_age(img_rgb, face.bbox)
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡
                face.landmarks_2d = self._get_2d_landmarks(img_rgb, face.bbox)
                face.landmarks_3d = self._get_3d_landmarks(img_rgb, face.bbox)
                
                # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
                self._draw_face_analysis(img, face)
            
            return faces
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {e}")
            traceback.print_exc()
            return []
    
    def _detect_faces(self, img_rgb, orig_w, orig_h):
        """ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SCRFD"""
        class Face:
            def __init__(self, bbox, score, kps=None):
                self.bbox = bbox  # [x1, y1, x2, y2]
                self.det_score = score
                self.kps = kps  # Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙØ§ØªÙŠØ­
                self.embedding = None
                self.gender = 0
                self.age = 25
                self.landmarks_2d = None
                self.landmarks_3d = None
        
        try:
            session = self.sessions["detection"]
            input_size = (640, 640)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
            img_resized = cv2.resize(img_rgb, input_size)
            img_normalized = img_resized.astype(np.float32) / 255.0
            img_normalized = (img_normalized - 0.5) / 0.5
            img_chw = np.transpose(img_normalized, (2, 0, 1))
            img_batch = np.expand_dims(img_chw, axis=0)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: img_batch})
            
            faces = []
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª (ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø®Ø±Ø¬Ø§Øª SCRFD)
            if len(outputs) >= 2:
                boxes = outputs[0]  # bounding boxes
                scores = outputs[1]  # confidence scores
                
                for i in range(len(scores)):
                    if scores[i] > 0.5:  # Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø©
                        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
                        scale_x = orig_w / input_size[0]
                        scale_y = orig_h / input_size[1]
                        
                        if boxes.shape[1] >= 4:
                            x1, y1, x2, y2 = boxes[i][:4]
                            x1 = int(x1 * scale_x)
                            y1 = int(y1 * scale_y)
                            x2 = int(x2 * scale_x)
                            y2 = int(y2 * scale_y)
                            
                            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
                            x1 = max(0, x1)
                            y1 = max(0, y1)
                            x2 = min(orig_w, x2)
                            y2 = min(orig_h, y2)
                            
                            face = Face([x1, y1, x2, y2], float(scores[i]))
                            faces.append(face)
                            print(f"ğŸ‘¤ ÙˆØ¬Ù‡ Ù…ÙƒØªØ´Ù: {face.bbox}, Ø«Ù‚Ø©: {scores[i]:.2f}")
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙˆØ¬ÙˆÙ‡ØŒ Ø¥Ø±Ø¬Ø§Ø¹ ÙˆØ¬Ù‡ Ø§ÙØªØ±Ø§Ø¶ÙŠ
            if len(faces) == 0:
                bbox_size = min(orig_w, orig_h) // 3
                x_center = orig_w // 2
                y_center = orig_h // 2
                bbox = [
                    max(0, x_center - bbox_size // 2),
                    max(0, y_center - bbox_size // 2),
                    min(orig_w, x_center + bbox_size // 2),
                    min(orig_h, y_center + bbox_size // 2)
                ]
                face = Face(bbox, 0.8)
                faces.append(face)
                print("âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¬Ù‡ Ø§ÙØªØ±Ø§Ø¶ÙŠ")
            
            return faces
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡: {e}")
            return []
    
    def _get_face_embedding(self, img_rgb, bbox):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ embedding Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GlintR100"""
        try:
            x1, y1, x2, y2 = bbox
            face_img = img_rgb[y1:y2, x1:x2]
            
            if face_img.size == 0:
                return np.random.randn(512).astype(np.float32)
            
            # ØªØ­Ø¶ÙŠØ± ØµÙˆØ±Ø© Ø§Ù„ÙˆØ¬Ù‡
            face_size = (112, 112)
            face_resized = cv2.resize(face_img, face_size)
            face_normalized = face_resized.astype(np.float32) / 255.0
            face_normalized = (face_normalized - 0.5) / 0.5
            face_chw = np.transpose(face_normalized, (2, 0, 1))
            face_batch = np.expand_dims(face_chw, axis=0)
            
            # ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù
            session = self.sessions["recognition"]
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: face_batch})
            
            if len(outputs) > 0:
                embedding = outputs[0].flatten()
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù€ embedding
                embedding = embedding / np.linalg.norm(embedding)
                return embedding.astype(np.float32)
            else:
                return np.random.randn(512).astype(np.float32)
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ embedding: {e}")
            return np.random.randn(512).astype(np.float32)
    
    def _analyze_gender_age(self, img_rgb, bbox):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø³ ÙˆØ§Ù„Ø¹Ù…Ø±"""
        try:
            x1, y1, x2, y2 = bbox
            face_img = img_rgb[y1:y2, x1:x2]
            
            if face_img.size == 0:
                return 0, 30
            
            # ØªØ­Ø¶ÙŠØ± ØµÙˆØ±Ø© Ø§Ù„ÙˆØ¬Ù‡
            face_size = (112, 112)
            face_resized = cv2.resize(face_img, face_size)
            face_normalized = face_resized.astype(np.float32) / 255.0
            face_normalized = (face_normalized - 0.5) / 0.5
            face_chw = np.transpose(face_normalized, (2, 0, 1))
            face_batch = np.expand_dims(face_chw, axis=0)
            
            # ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ù†Ø³ ÙˆØ§Ù„Ø¹Ù…Ø±
            session = self.sessions["genderage"]
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: face_batch})
            
            if len(outputs) >= 2:
                gender_logits = outputs[0]
                age_output = outputs[1]
                
                # ØªÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ù†Ø³ (0 = Ø£Ù†Ø«Ù‰, 1 = Ø°ÙƒØ±)
                gender = 1 if gender_logits[0][0] < gender_logits[0][1] else 0
                age = int(age_output[0][0])
                
                return gender, max(1, min(100, age))
            else:
                return 0, 30
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø³ ÙˆØ§Ù„Ø¹Ù…Ø±: {e}")
            return 0, 30
    
    def _get_2d_landmarks(self, img_rgb, bbox):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 2D"""
        try:
            x1, y1, x2, y2 = bbox
            face_img = img_rgb[y1:y2, x1:x2]
            
            if face_img.size == 0:
                return np.zeros((106, 2), dtype=np.float32)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±Ø©
            face_size = (192, 192)
            face_resized = cv2.resize(face_img, face_size)
            face_normalized = face_resized.astype(np.float32) / 255.0
            face_normalized = (face_normalized - 0.5) / 0.5
            face_chw = np.transpose(face_normalized, (2, 0, 1))
            face_batch = np.expand_dims(face_chw, axis=0)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            session = self.sessions["landmarks_2d"]
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: face_batch})
            
            if len(outputs) > 0:
                landmarks = outputs[0].reshape(-1, 2)
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£ØµÙ„ÙŠ
                scale_x = (x2 - x1) / face_size[0]
                scale_y = (y2 - y1) / face_size[1]
                landmarks[:, 0] = landmarks[:, 0] * scale_x + x1
                landmarks[:, 1] = landmarks[:, 1] * scale_y + y1
                return landmarks
            else:
                return np.zeros((106, 2), dtype=np.float32)
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 2D: {e}")
            return np.zeros((106, 2), dtype=np.float32)
    
    def _get_3d_landmarks(self, img_rgb, bbox):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 3D"""
        try:
            x1, y1, x2, y2 = bbox
            face_img = img_rgb[y1:y2, x1:x2]
            
            if face_img.size == 0:
                return np.zeros((68, 3), dtype=np.float32)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±Ø©
            face_size = (192, 192)
            face_resized = cv2.resize(face_img, face_size)
            face_normalized = face_resized.astype(np.float32) / 255.0
            face_normalized = (face_normalized - 0.5) / 0.5
            face_chw = np.transpose(face_normalized, (2, 0, 1))
            face_batch = np.expand_dims(face_chw, axis=0)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            session = self.sessions["landmarks_3d"]
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: face_batch})
            
            if len(outputs) > 0:
                landmarks = outputs[0].reshape(-1, 3)
                return landmarks
            else:
                return np.zeros((68, 3), dtype=np.float32)
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 3D: {e}")
            return np.zeros((68, 3), dtype=np.float32)
    
    def _draw_face_analysis(self, img, face):
        """Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©"""
        try:
            # Ø±Ø³Ù… bounding box
            x1, y1, x2, y2 = [int(coord) for coord in face.bbox]
            color = (0, 255, 0) if face.gender == 1 else (255, 0, 255)  # Ø£Ø²Ø±Ù‚ Ù„Ù„Ø°ÙƒØ±, ÙˆØ±Ø¯ÙŠ Ù„Ù„Ø£Ù†Ø«Ù‰
            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
            
            # Ø±Ø³Ù… Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 2D
            if face.landmarks_2d is not None:
                for point in face.landmarks_2d:
                    x, y = int(point[0]), int(point[1])
                    cv2.circle(img, (x, y), 1, (0, 255, 255), -1)
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Øµ
            label = f"{'Ø°ÙƒØ±' if face.gender == 1 else 'Ø£Ù†Ø«Ù‰'} {face.age}Ø³Ù†Ø©"
            cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø±Ø³Ù…: {e}")

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("ğŸ”§ Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ AntelopeV2...")
face_analyzer = AntelopeV2FaceAnalysis()
init_success = face_analyzer.prepare()

if init_success:
    print("ğŸ‰ Ù†Ù…ÙˆØ°Ø¬ AntelopeV2 Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")
else:
    print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")

# ===========================
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙˆÙŠØ¨
# ===========================
HTML_PAGE = """
<!DOCTYPE html>
<html lang="ar">
<head>
    <meta charset="UTF-8">
    <title>ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¬ÙˆÙ‡ - AntelopeV2</title>
    <style>
        body {font-family: Arial; text-align:center; background:#f5f5f5;}
        h2 {color:#333;}
        form {margin:30px auto; padding:20px; background:white; border-radius:15px; width:350px; box-shadow:0 0 10px #ccc;}
        input[type=file]{margin:10px;}
        img {margin-top:20px; max-width:400px; border-radius:10px;}
        .info {background:#fff; display:inline-block; margin-top:20px; padding:15px; border-radius:10px; box-shadow:0 0 5px #aaa;}
        .error {background:#ffe6e6; color:#d00; padding:15px; border-radius:10px;}
        .success {background:#e6ffe6; color:#060; padding:10px; border-radius:10px;}
        .male {color: blue; font-weight: bold;}
        .female {color: pink; font-weight: bold;}
        .stats {background:#f0f8ff; padding:10px; border-radius:5px; margin:10px;}
        .model-info {background:#fffacd; padding:10px; border-radius:5px; margin:10px;}
        .landmarks {background:#f0fff0; padding:10px; border-radius:5px; margin:10px;}
    </style>
</head>
<body>
    <div class="success">
        <h2>ğŸ§  Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¬ÙˆÙ‡ - AntelopeV2</h2>
        <p>Ø£Ø­Ø¯Ø« Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¬ÙˆÙ‡</p>
    </div>
    
    {% if model_loaded %}
    <div class="success">
        <p>âœ… Ù†Ù…ÙˆØ°Ø¬ AntelopeV2 Ù…Ø­Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­</p>
    </div>
    {% else %}
    <div class="error">
        <p>âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø­Ù…Ù„</p>
    </div>
    {% endif %}
    
    <div class="model-info">
        <h4>ğŸ“Š Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:</h4>
        <p>â€¢ SCRFD - ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡</p>
        <p>â€¢ GlintR100 - Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡</p>
        <p>â€¢ GenderAge - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø³ ÙˆØ§Ù„Ø¹Ù…Ø±</p>
        <p>â€¢ 2D106 - Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 2D</p>
        <p>â€¢ 3D68 - Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ 3D</p>
    </div>
    
    <form method="POST" enctype="multipart/form-data">
        <input type="file" name="image" accept="image/*" required>
        <br><br>
        <button type="submit">ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©</button>
    </form>
    
    {% if error %}
    <div class="error">
        <h3>âš ï¸ Ø®Ø·Ø£:</h3>
        <p>{{ error }}</p>
    </div>
    {% endif %}
    
    {% if result %}
    <div class="info">
        <h3>ğŸ‘¤ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:</h3>
        <div class="stats">
            <p class="{{ 'male' if result.gender == 1 else 'female' }}">
                ğŸš¹ğŸšº Ø§Ù„Ø¬Ù†Ø³: <strong>{{ 'Ø°ÙƒØ±' if result.gender == 1 else 'Ø£Ù†Ø«Ù‰' }}</strong>
            </p>
            <p>ğŸ‚ Ø§Ù„Ø¹Ù…Ø±: <strong>{{ result.age }} Ø³Ù†Ø©</strong></p>
            <p>ğŸ‘¥ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ¬ÙˆÙ‡: <strong>{{ result.faces }}</strong></p>
            <p>ğŸ¯ Ø§Ù„Ø«Ù‚Ø©: <strong>{{ "%.1f"|format(result.confidence * 100) }}%</strong></p>
        </div>
        {% if result.landmarks_2d %}
        <div class="landmarks">
            <p>ğŸ“ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡: <strong>106 Ù†Ù‚Ø·Ø© 2D + 68 Ù†Ù‚Ø·Ø© 3D</strong></p>
        </div>
        {% endif %}
        <img src="{{ image_url }}" alt="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø­Ù„Ù„Ø©">
    </div>
    {% endif %}
</body>
</html>
"""

@app.route("/", methods=["GET", "POST"])
def index():
    try:
        if request.method == "POST":
            if not face_analyzer.initialized:
                return render_template_string(HTML_PAGE, 
                    error="Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ø¬Ø§Ù‡Ø²",
                    model_loaded=False)

            file = request.files["image"]
            if file:
                file_data = file.read()
                img_array = np.frombuffer(file_data, np.uint8)
                img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                
                if img is None:
                    return render_template_string(HTML_PAGE, 
                        error="ØªØ¹Ø°Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©",
                        model_loaded=face_analyzer.initialized)

                print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ AntelopeV2...")
                faces = face_analyzer.get(img)
                print(f"ğŸ“Š ÙˆØ¬ÙˆÙ‡ Ù…ÙƒØªØ´ÙØ©: {len(faces)}")

                if len(faces) == 0:
                    cv2.imwrite("uploaded.jpg", img)
                    return render_template_string(HTML_PAGE, 
                        error="Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ¬ÙˆÙ‡",
                        model_loaded=face_analyzer.initialized)

                face = faces[0]
                result = {
                    'gender': face.gender,
                    'age': face.age,
                    'faces': len(faces),
                    'confidence': face.det_score,
                    'landmarks_2d': face.landmarks_2d is not None,
                    'landmarks_3d': face.landmarks_3d is not None
                }
                
                cv2.imwrite("uploaded.jpg", img)
                return render_template_string(HTML_PAGE, 
                    result=result, 
                    image_url="/image",
                    model_loaded=face_analyzer.initialized)
        
        return render_template_string(HTML_PAGE, 
            result=None, 
            model_loaded=face_analyzer.initialized)
    
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
        return render_template_string(HTML_PAGE, 
            error=str(e),
            model_loaded=face_analyzer.initialized)

@app.route("/image")
def serve_image():
    try:
        return send_file("uploaded.jpg", mimetype="image/jpeg")
    except:
        return "Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©", 404

@app.route("/models")
def list_models():
    """Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    models_info = {}
    for name, url in MODEL_URLS.items():
        models_info[name] = {
            "url": url,
            "loaded": name in face_analyzer.sessions
        }
    return jsonify(models_info)

if __name__ == "__main__":
    port = int(os.getenv("PORT", 5000))
    print(f"ğŸš€ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰: http://0.0.0.0:{port}")
    app.run(host="0.0.0.0", port=port, debug=False)
